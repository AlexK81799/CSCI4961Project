{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet neural-structured-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is Tensorflow's Neural Structure Learning<br>\n",
    "https://www.tensorflow.org/neural_structured_learning<br><br>\n",
    "This is the link with a more in depth look at how it works<br>\n",
    "https://blog.tensorflow.org/2019/09/introducing-neural-structured-learning.html<br><br>\n",
    "\n",
    "\n",
    "The basic idea of this library is that it is a new way to train neural networks by using structured signals in conjunction with feature inputs.\n",
    "Structured signals use graphs to show how samples are related by stating their neighbors and use these relations as input data, where in training \n",
    "it is minimizing supervised loss and neighbor loss. This relates to adversarial training, since we can create structures dynamically for input samples\n",
    "by constructing the structures with adversarial perturbation. <br><br>\n",
    "\n",
    "This notebook will be following the tutorial provided by Tensorflow.<br>\n",
    "https://www.tensorflow.org/neural_structured_learning/tutorials/adversarial_keras_cnn_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import neural_structured_learning as nsl\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the hyper params defined for model training and evaluating\n",
    "class HParams(object):\n",
    "  def __init__(self):\n",
    "    self.input_shape = [28, 28, 1]\n",
    "    self.num_classes = 10\n",
    "    self.conv_filters = [32, 64, 64]\n",
    "    self.kernel_size = (3, 3)\n",
    "    self.pool_size = (2, 2)\n",
    "    self.num_fc_units = [64]\n",
    "    self.batch_size = 32\n",
    "    self.epochs = 5\n",
    "    self.adv_multiplier = 0.2\n",
    "    self.adv_step_size = 0.2\n",
    "    self.adv_grad_norm = 'infinity'\n",
    "\n",
    "HPARAMS = HParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we load the MNIST dataset (Goal later on is to adapt this code with other datasets)\n",
    "datasets = tfds.load('mnist')\n",
    "\n",
    "train_dataset = datasets['train']\n",
    "test_dataset = datasets['test']\n",
    "\n",
    "IMAGE_INPUT_NAME = 'image'\n",
    "LABEL_INPUT_NAME = 'label'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we normalize the pixel values to [0, 1]\n",
    "def normalize(features):\n",
    "  features[IMAGE_INPUT_NAME] = tf.cast(\n",
    "      features[IMAGE_INPUT_NAME], dtype=tf.float32) / 255.0\n",
    "  return features\n",
    "\n",
    "def convert_to_tuples(features):\n",
    "  return features[IMAGE_INPUT_NAME], features[LABEL_INPUT_NAME]\n",
    "\n",
    "def convert_to_dictionaries(image, label):\n",
    "  return {IMAGE_INPUT_NAME: image, LABEL_INPUT_NAME: label}\n",
    "\n",
    "# shuffling training set and batching then converting the examples to feature tuples (image, label) for training the base model\n",
    "train_dataset = train_dataset.map(normalize).shuffle(10000).batch(HPARAMS.batch_size).map(convert_to_tuples)\n",
    "test_dataset = test_dataset.map(normalize).batch(HPARAMS.batch_size).map(convert_to_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the base model\n",
    "def build_base_model(hparams):\n",
    "  \"\"\"Builds a model according to the architecture defined in `hparams`.\"\"\"\n",
    "  inputs = tf.keras.Input(\n",
    "      shape=hparams.input_shape, dtype=tf.float32, name=IMAGE_INPUT_NAME)\n",
    "\n",
    "  x = inputs\n",
    "  for i, num_filters in enumerate(hparams.conv_filters):\n",
    "    x = tf.keras.layers.Conv2D(\n",
    "        num_filters, hparams.kernel_size, activation='relu')(\n",
    "            x)\n",
    "    if i < len(hparams.conv_filters) - 1:\n",
    "      # max pooling between convolutional layers\n",
    "      x = tf.keras.layers.MaxPooling2D(hparams.pool_size)(x)\n",
    "  x = tf.keras.layers.Flatten()(x)\n",
    "  for num_units in hparams.num_fc_units:\n",
    "    x = tf.keras.layers.Dense(num_units, activation='relu')(x)\n",
    "  pred = tf.keras.layers.Dense(hparams.num_classes, activation='softmax')(x)\n",
    "  model = tf.keras.Model(inputs=inputs, outputs=pred)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = build_base_model(HPARAMS)\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',\n",
    "                   metrics=['acc'])\n",
    "base_model.fit(train_dataset, epochs=HPARAMS.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = base_model.evaluate(test_dataset)\n",
    "named_results = dict(zip(base_model.metrics_names, results))\n",
    "print('\\naccuracy:', named_results['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_config = nsl.configs.make_adv_reg_config(\n",
    "    multiplier=HPARAMS.adv_multiplier,\n",
    "    adv_step_size=HPARAMS.adv_step_size,\n",
    "    adv_grad_norm=HPARAMS.adv_grad_norm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_adv_model = build_base_model(HPARAMS)\n",
    "adv_model = nsl.keras.AdversarialRegularization(\n",
    "    base_adv_model,\n",
    "    label_keys=[LABEL_INPUT_NAME],\n",
    "    adv_config=adv_config\n",
    ")\n",
    "\n",
    "train_set_for_adv_model = train_dataset.map(convert_to_dictionaries)\n",
    "test_set_for_adv_model = test_dataset.map(convert_to_dictionaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',\n",
    "                   metrics=['acc'])\n",
    "adv_model.fit(train_set_for_adv_model, epochs=HPARAMS.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = adv_model.evaluate(test_set_for_adv_model)\n",
    "named_results = dict(zip(adv_model.metrics_names, results))\n",
    "print('\\naccuracy:', named_results['sparse_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_model = nsl.keras.AdversarialRegularization(\n",
    "    base_model,\n",
    "    label_keys=[LABEL_INPUT_NAME],\n",
    "    adv_config=adv_config)\n",
    "reference_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_eval = {\n",
    "    'base': base_model,\n",
    "    'adv-regularized': adv_model.base_model\n",
    "}\n",
    "metrics = {\n",
    "    name: tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "    for name in models_to_eval.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "perturbed_images, labels, predictions = [], [], []\n",
    "\n",
    "for batch in test_set_for_adv_model:\n",
    "  perturbed_batch = reference_model.perturb_on_batch(batch)\n",
    "  # Clipping makes perturbed examples have the same range as regular ones.\n",
    "  perturbed_batch[IMAGE_INPUT_NAME] = tf.clip_by_value(                          \n",
    "      perturbed_batch[IMAGE_INPUT_NAME], 0.0, 1.0)\n",
    "  y_true = perturbed_batch.pop(LABEL_INPUT_NAME)\n",
    "  perturbed_images.append(perturbed_batch[IMAGE_INPUT_NAME].numpy())\n",
    "  labels.append(y_true.numpy())\n",
    "  predictions.append({})\n",
    "  for name, model in models_to_eval.items():\n",
    "    y_pred = model(perturbed_batch)\n",
    "    metrics[name](y_true, y_pred)\n",
    "    predictions[-1][name] = tf.argmax(y_pred, axis=-1).numpy()\n",
    "\n",
    "for name, metric in metrics.items():\n",
    "  print('%s model accuracy: %f' % (name, metric.result().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_index = 0\n",
    "\n",
    "batch_image = perturbed_images[batch_index]\n",
    "batch_label = labels[batch_index]\n",
    "batch_pred = predictions[batch_index]\n",
    "\n",
    "batch_size = HPARAMS.batch_size\n",
    "n_col = 4\n",
    "n_row = (batch_size + n_col - 1) / n_col\n",
    "\n",
    "print('accuracy in batch %d:' % batch_index)\n",
    "for name, pred in batch_pred.items():\n",
    "  print('%s model: %d / %d' % (name, np.sum(batch_label == pred), batch_size))\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "for i, (image, y) in enumerate(zip(batch_image, batch_label)):\n",
    "  y_base = batch_pred['base'][i]\n",
    "  y_adv = batch_pred['adv-regularized'][i]\n",
    "  plt.subplot(n_row, n_col, i+1)\n",
    "  plt.title('true: %d, base: %d, adv: %d' % (y, y_base, y_adv))\n",
    "  plt.imshow(tf.keras.preprocessing.image.array_to_img(image), cmap='gray')\n",
    "  plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "93eb259384ccf2bcbbf5dc67d78e51a41bc3a8a192fd75a7646ba938771272a8"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}