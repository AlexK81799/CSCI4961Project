\documentclass{article}

% If you're new to LaTeX, here's some short tutorials:
% https://www.overleaf.com/learn/latex/Learn_LaTeX_in_30_minutes
% https://en.wikibooks.org/wiki/LaTeX/Basics

% Formatting
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage[titletoc,title]{appendix}
\usepackage{url}

% Math
% https://www.overleaf.com/learn/latex/Mathematical_expressions
% https://en.wikibooks.org/wiki/LaTeX/Mathematics
\usepackage{amsmath,amsfonts,amssymb,mathtools}

% Images
% https://www.overleaf.com/learn/latex/Inserting_Images
% https://en.wikibooks.org/wiki/LaTeX/Floats,_Figures_and_Captions
\usepackage{graphicx,float}

% Tables
% https://www.overleaf.com/learn/latex/Tables
% https://en.wikibooks.org/wiki/LaTeX/Tables

% Algorithms
% https://www.overleaf.com/learn/latex/algorithms
% https://en.wikibooks.org/wiki/LaTeX/Algorithms
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{algorithmic}

% Code syntax highlighting
% https://www.overleaf.com/learn/latex/Code_Highlighting_with_minted
\usepackage{minted}
\usemintedstyle{borland}

% References
% https://www.overleaf.com/learn/latex/Bibliography_management_in_LaTeX
% https://en.wikibooks.org/wiki/LaTeX/Bibliography_Management
\usepackage{biblatex}
\addbibresource{references.bib}

% Title content
\title{CSCI 4261/6961 Pedagogical Project Assignment}
\author{Haniel Campos, Gretchen Forbush, Alex Kim}
\date{December 2, 2020}

\begin{document}

\maketitle

% Introduction and Overview
\section{Introduction and Overview}

% Example Subsection
\subsection{Generating Adversarial Attacks}
This is a subsection.
In the realm of adversarial attacks, there are two distinct types: targeted and untargeted. Given target model, M, and image, I, with true class, X:
\begin{itemize}
    \item Untargeted Attacks - goal: have M misclassify I as a class other than X
    \begin{itemize}
        \item Benefit: faster
        \item Drawback: not as reliable
    \end{itemize}
    \item Targeted Attacks - goal: have M misclassify I as a target class, Y
    \begin{itemize}
        \item Benefit: more successful attack method
        \item Drawback: costly (time)
    \end{itemize}
\end{itemize}

\subsection{Benign vs. Attack Systems}
As we have seen in our studies, benign systems (those without attacks) have a variation of the following goal
$$argmin_f \sum_{x_i\in D} l(f(x_i),y_i)$$
where $f$ is a prediction, $l$ is the loss function, $D$ is the input data, $x_i$ is an element in the data, and $y_i$ is the target label

In an attack, the system is changed slightly by these two steps:
\begin{enumerate}
    \item The input data ($D$) is changed from $D$ to $D'$
    \item  A goal is set for the attack so that $f(x_i)$ no longer outputs $y_i$. This will change the loss inputs from $l(f(x_i),y_i)$ to $l(f(x_i),y'_i)$ with $y_i \neq y'_i$.
\end{enumerate}


\subsection{Adversarial Perturbation}
Step 1 can be performed through a method called "adversarial perturbation." 
\begin{itemize}
    \item Untargeted attacks: maximize loss between $f(x)$ and $f(x')$ until the prediction is incorrect
    \item Targeted attacks: maximize loss between $f(x)$ and $f(x')$ AND minimize the loss between $f(x')$ and $y'$ until $f(x') = y'$
\end{itemize}

There are two types of adversarial perturbation:
\begin{enumerate}
    \item Single-step - add noise once and be done
e.g. Fast Gradient Sign Method
$$\text{Untargeted: }x' = x + \epsilon.sign(\nabla_xl(x,y))$$
$$\text{Targeted: }x' = x - \epsilon.sign(\nabla_xl(x,y_{target}))$$
\begin{itemize}
    \item Benefit: fast
    \item Drawbacks: often easier to detect; focuses on maximizing the loss over minimizing perturbation
\end{itemize}
\item Multi-step - make a small perturbation at each iteration
e.g. Fast Gradient Sign Method
$$\text{Untargeted: }x_0' = x \implies x_{N+1}' = Clip_{x,\epsilon}\{x_N' + \alpha.sign(\nabla_xl(x_n', y)\} $$
$$\text{Targeted: }x_0' = x \implies x_{N+1}' = Clip_{x,\epsilon}\{x_N' + \alpha.sign(\nabla_xl(x_n', y_{target})\} $$
\begin{itemize}
    \item Benefit: more successful
    \item Drawbacks: more expensive (computationaly)
\end{itemize}
\end{enumerate}

\section{Problem Set}

Using the starter code from \path{adv_problemset_starter.ipynb}, complete the following tasks:
\begin{enumerate}
    \item Write a function \texttt{adversarial\_attack(images, labels, eps)} that performs a single-step untargeted adversarial attack on the CIFAR10 images provided.
    \begin{itemize}
        \item $\texttt{images}$ is the set of input images provided to the model
        \item $\texttt{labels}$ is the set of true labels provided by the dataset
        \item $\texttt{eps}$ is a hyperparameter used in the calculations
    \end{itemize}
    \textbf{HINTS:}
    \begin{itemize}
        \item Use the loss object \texttt{keras.losses.CategoricalCrossentropy()} to calculate the loss for each iteration. Make sure to indicate that the loss is being calculated from the logits. See \url{https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy} for more details.
        \item Consider converting the inputs to \texttt{tensor}s to take advantage of certain built-in functions (e.g. tf.GradientTape()). Just do not forget to convert back to \texttt{nparray}s on output!
    \end{itemize}
    \item Run the adversarial attack function on the training data with different values of epsilon to determine an appropriate value.
    \begin{itemize}
        \item Evaluate the model on these attack images against the true labels and output the base accuracy. 
        \item Use the model to predict the outputs of the adversarial images.
        \item Use the pre-defined method \texttt{display\_images(images, predicted\_labels, true\_labels)} to display some of the predictions.
    \end{itemize}
    \item Write a function \texttt{multistep\_adversarial\_attack(images, labels, eps, a, T)} that performs a multistep untargeted adversarial attack on the images provided.
    \begin{itemize}
        \item $\texttt{images}$ is the set of input images provided to the model
        \item $\texttt{labels}$ is the set of true labels provided by the dataset
        \item $\texttt{eps}$ is a hyperparameter used in the calculations
        \item \texttt{a} is a parameter that defines the step-size by $\alpha_t = (1 - a*t)^{-1}$
        \item \texttt{T} is the number of steps taken per input image
    \end{itemize}
    \textbf{HINTS:}
    \begin{itemize}
        \item Use the same loss object \texttt{keras.losses.CategoricalCrossentropy()} to calculate the loss for each iteration. 
        \item You should not have to change too much from the single-step algorithm!
    \end{itemize}
    \item Run the adversarial attack function on the training data with different values of epsilon, a, and T to determine an appropriate value.
    \begin{itemize}
        \item Evaluate the model on these attack images against the true labels and output the base accuracy. 
        \item Use the model to predict the outputs of the adversarial images.
        \item Use the pre-defined method \texttt{display\_images(images, predicted\_labels, true\_labels)} to display some of the predictions.
    \end{itemize}
    \item Compare the results from step 2 to those from step 4. 
    \begin{itemize}
        \item Are they as you expected?
        \item What is the runtime complexity of both of your algorithms?
        \item How might you expect a targeted attack to perform compared to the untargeted ones you implemented? 
        \item What would you expect the time complexity of a targeted (single-step and multistep) attack to be?
    \end{itemize}
    
\end{enumerate}



\end{document}
